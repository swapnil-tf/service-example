from typing import Any, Dict, List, Optional

from langchain.llms.base import LLM
from langchain.pydantic_v1 import Extra, Field, root_validator

from servicefoundry.langchain.utils import (
    ModelParameters,
    requests_retry_session,
    validate_tfy_environment,
)
from servicefoundry.logger import logger


class TrueFoundryLLM(LLM):
    """`TrueFoundry LLM Gateway` completion models API.

    To use, you must have the environment variable ``TFY_API_KEY`` set with your API key and ``TFY_HOST`` set with your host or pass it as a named parameter to the constructor.
    """

    model: str = Field(description="The model to use for completion.")
    """The model to use for completion."""
    tfy_llm_gateway_url: Optional[str] = Field(default=None)
    """TrueFoundry LLM Gateway endpoint URL. Automatically inferred from env var `TFY_LLM_GATEWAY_URL` if not provided."""
    tfy_api_key: Optional[str] = Field(default=None)
    """TrueFoundry API Key. Automatically inferred from env var `TFY_API_KEY` if not provided."""
    model_parameters: Optional[dict] = Field(default_factory=dict)
    """Model parameters"""
    request_timeout: int = Field(default=30)
    """The timeout for the request in seconds."""
    max_retries: int = Field(default=5)
    """The number of retries for HTTP requests."""
    retry_backoff_factor: float = Field(default=0.3)
    """The backoff factor for exponential backoff during retries."""

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid
        allow_population_by_field_name = True

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        values = validate_tfy_environment(values)
        if not values["tfy_api_key"]:
            raise ValueError(
                f"Did not find `tfy_api_key`, please add an environment variable"
                f" `TFY_API_KEY` which contains it, or pass"
                f"  `tfy_api_key` as a named parameter."
            )
        if not values["tfy_llm_gateway_url"]:
            raise ValueError(
                f"Did not find `tfy_llm_gateway_url`, please add an environment variable"
                f" `TFY_LLM_GATEWAY_URL` which contains it, or pass"
                f"  `tfy_llm_gateway_url` as a named parameter."
            )
        return values

    @property
    def _llm_type(self) -> str:
        """Return type of llm model."""
        return "truefoundry-llm"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> str:
        """Call out to the deployed model

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.

        Returns:
            The string generated by the model.

        Example:
            .. code-block:: python

                response = model("I have a joke for you...")
        """

        payload = {**self.model_parameters} if self.model_parameters else {}
        if stop:
            payload["stop_sequences"] = stop

        payload["prompt"] = prompt
        payload["model"] = self.model

        session = requests_retry_session(
            retries=self.max_retries, backoff_factor=self.retry_backoff_factor
        )

        url = f"{self.tfy_llm_gateway_url}/openai/completions"
        logger.debug(f"Completion using - model: {self.model} at endpoint: {url}")
        response = session.post(
            url=url,
            json=payload,
            headers={
                "Authorization": f"Bearer {self.tfy_api_key}",
            },
            timeout=self.request_timeout,
        )
        response.raise_for_status()
        output = response.json()
        return output["choices"][0]["text"]
